{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#0000CD'>Automated Machine Learning for Concrete Strength Prediction using Apache Spark MLlib</font>\n",
    "\n",
    "### In this project, we build an AutoML system from scratch to predict concrete compressive strength using PySpark's MLlib. We will experiment with various regression algorithms and optimize hyperparameters to find the best-performing model.\n",
    "\n",
    "#### Algorithms used:\n",
    "* Linear Regression\n",
    "* Descision Tree\n",
    "* Random Forest\n",
    "* GBT\n",
    "* Isotonic Regression\n",
    "\n",
    "#### The dataset can be found here:[dataset](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Context\n",
    "sc = SparkContext(appName = \"AutoML_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AutoML_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16218f6faf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = spark.read.csv('data/dataset.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "|cement| slag|flyash|water|superplasticizer|coarseaggregate|fineaggregate|age|csMPa|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1040.0|        676.0| 28|79.99|\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1055.0|        676.0| 28|61.89|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|270|40.27|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|365|41.05|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5|360| 44.3|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 90|47.03|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0|365| 43.7|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|36.45|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 28|45.85|\n",
      "| 475.0|  0.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|39.29|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame \n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>90</td>\n",
       "      <td>47.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>380.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>43.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>380.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>28</td>\n",
       "      <td>36.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>28</td>\n",
       "      <td>45.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>28</td>\n",
       "      <td>39.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "5   266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "6   380.0   95.0     0.0  228.0               0.0            932.0   \n",
       "7   380.0   95.0     0.0  228.0               0.0            932.0   \n",
       "8   266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "9   475.0    0.0     0.0  228.0               0.0            932.0   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  \n",
       "5          670.0   90  47.03  \n",
       "6          594.0  365  43.70  \n",
       "7          594.0   28  36.45  \n",
       "8          670.0   28  45.85  \n",
       "9          594.0   28  39.29  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Automation Module\n",
    "\n",
    "**1- Automate Data Preparation**   \n",
    "**2- autoML module**\n",
    "\n",
    "MLlib requires all dataframe input columns to be vectorized. Let's create a Python function that will automate our data preparation work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values (if they exist). We will focus on Machine Learning in this project, but always remember to check for missing values and possible outliers when preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing missing values: 1030\n",
      "Number of rows after removing missing values: 1030\n"
     ]
    }
   ],
   "source": [
    "# Removing null values\n",
    "data2 = data.na.drop()\n",
    "print('Number of rows before removing missing values:', data.count())\n",
    "print('Number of rows after removing missing values:', data2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation function\n",
    "def func_data_prep(df,\n",
    "                   input_variable,\n",
    "                   output_variable,\n",
    "                   treat_outliers = True,\n",
    "                   standardize_data = True):\n",
    "\n",
    "     # Let's generate a new dataframe, renaming the argument that represents the output variable.\n",
    "     # Apache Spark wants the final df to have the column name \"features\" and \"label\"\n",
    "    new_df = df.withColumnRenamed(output_variable, 'label')\n",
    "    \n",
    "    # Convert the target variable to numeric type as float (encoding)\n",
    "    if str(new_df.schema['label'].dataType) != 'IntegerType':\n",
    "        new_df = new_df.withColumn(\"label\", new_df[\"label\"].cast(FloatType()))\n",
    "    \n",
    "    # Variable Control\n",
    "    numerical_variables = []\n",
    "    categorical_variables = []\n",
    "    \n",
    "    # If there is string type input variables, convert them to numeric type\n",
    "    for column in input_variable:\n",
    "        \n",
    "        # Check if the variable is string\n",
    "        if str(new_df.schema[column].dataType) == 'StringType':\n",
    "            \n",
    "            # We define the variable with a suffix\n",
    "            new_column_name = column + \"_num\"\n",
    "            \n",
    "            # Add to the list of categorical variables\n",
    "            categorical_variables.append(new_column_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # If it is not a string variable, then it is numeric and we add it to the corresponding list\n",
    "            numerical_variables.append(column)\n",
    "            \n",
    "            # We place the data in the indexed variables dataframe\n",
    "            df_indexed = new_df\n",
    "            \n",
    "    # If the dataframe has data of type string, we apply indexing\n",
    "    if len(categorical_variables) != 0: \n",
    "        \n",
    "        # Loop through columns\n",
    "        for column in new_df:\n",
    "            \n",
    "            # If the variable is of type string, we create, train and apply the indexer\n",
    "            if str(new_df.schema[column].dataType) == 'StringType':\n",
    "                \n",
    "                # Create the indexer\n",
    "                indexer = StringIndexer(inputCol = column, outputCol = column + \"_num\") \n",
    "                \n",
    "                # Train and apply the indexer\n",
    "                df_indexed = indexer.fit(new_df).transform(new_df)\n",
    "    else:\n",
    "        \n",
    "        # If we no longer have categorical variables, then we place the data in the indexed variables dataframe\n",
    "        df_indexed = new_df\n",
    "        \n",
    "    # If it is necessary to treat outliers, we do now\n",
    "    if treat_outliers == True:\n",
    "        print(\"\\nApplying outlier treatment...\")\n",
    "        \n",
    "        # Dictionary\n",
    "        d = {}\n",
    "        \n",
    "        # Dictionary of quartiles of indexed dataframe variables (numeric variables only)\n",
    "        for col in numerical_variables: \n",
    "            d[col] = df_indexed.approxQuantile(col,[0.01, 0.99], 0.25) \n",
    "        \n",
    "        # Apply the transformation depending on the distribution of each variable\n",
    "        for col in numerical_variables:\n",
    "            \n",
    "            # Extract the asymmetry from the data and use this to handle outliers\n",
    "            skew = df_indexed.agg(skewness(df_indexed[col])).collect() \n",
    "            skew = skew[0][0]\n",
    "            \n",
    "            # We check the asymmetry and then apply:\n",
    "            \n",
    "            # Log transformation + 1 if skewness is positive\n",
    "            if skew > 1:\n",
    "                indexed = df_indexed.withColumn(col, log(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] ) + 1).alias(col))\n",
    "                print(\"\\nA variable \" + col + \" was treated for positive (right) skewness with skew =\", skew)\n",
    "            \n",
    "            # Exponential transformation if the asymmetry is negative\n",
    "            elif skew < -1:\n",
    "                indexed = df_indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] )).alias(col))\n",
    "                print(\"\\nA variable \" + col + \" was treated for negative (left) skewness with skew =\", skew)\n",
    "                \n",
    "            # **Asymmetry between -1 and 1 we do not need to apply transformation to the data**\n",
    "\n",
    "    # Vectorization\n",
    "    \n",
    "    # Final list of attributes\n",
    "    attributes_list = numerical_variables + categorical_variables\n",
    "    \n",
    "    # Creates the vectorizer for the attributes\n",
    "    vectorizer = VectorAssembler(inputCols = attributes_list, outputCol = 'features')\n",
    "    \n",
    "    # Apply the vectorizer to the dataset\n",
    "    vectorized_data = vectorizer.transform(df_indexed).select('features', 'label')\n",
    "    \n",
    "    # Standardize the data by placing them on the same scale\n",
    "    if standardize_data == True:\n",
    "        print(\"\\nStandardizing the dataset to the range 0 to 1...\")\n",
    "        \n",
    "        # Create the scaler\n",
    "        scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
    "\n",
    "        # Calculates summary statistics and generates the standardizer\n",
    "        global scalerModel\n",
    "        scalerModel = scaler.fit(vectorized_data)\n",
    "\n",
    "        # Standardizes variables to the range [min, max]\n",
    "        standardized_data = scalerModel.transform(vectorized_data)\n",
    "        \n",
    "        # Generates the final data\n",
    "        final_data = standardized_data.select('label', 'scaledFeatures')\n",
    "        \n",
    "        # Rename columns (required by Spark)\n",
    "        final_data = final_data.withColumnRenamed('scaledFeatures', 'features')\n",
    "        \n",
    "        print(\"\\nProcess concluded!\")\n",
    "\n",
    "    # If the flag is set to False, then we have not standardized the data\n",
    "    else:\n",
    "        print(\"\\nThe data will not be standardized because the standardize_dados flag has the value False.\")\n",
    "        final_data = vectorized_data\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the data preparation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input variables (all but the last)\n",
    "input_variable = data.columns[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "output_variable = data.columns[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying outlier treatment...\n",
      "\n",
      "A variable age was treated for positive (right) skewness with skew = 3.2644145354168086\n",
      "\n",
      "Standardizing the dataset to the range 0 to 1...\n",
      "\n",
      "Process concluded!\n"
     ]
    }
   ],
   "source": [
    "# Apply the function\n",
    "final_data = func_data_prep(data, input_variable, output_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                      |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|79.99|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.6947674418604651,0.20572002007024587,0.07417582417582418]               |\n",
      "|61.89|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.7383720930232558,0.20572002007024587,0.07417582417582418]               |\n",
      "|40.27|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.739010989010989]                    |\n",
      "|41.05|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|44.3 |[0.22054794520547943,0.3683917640511965,0.0,0.560702875399361,0.0,0.5156976744186046,0.58078273958856,0.9862637362637363]     |\n",
      "|47.03|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.24450549450549453]|\n",
      "|43.7 |[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|36.45|[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.07417582417582418]                  |\n",
      "|45.85|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.07417582417582418]|\n",
      "|39.29|(8,[0,3,5,7],[0.8515981735159817,0.8482428115015974,0.3808139534883721,0.07417582417582418])                                  |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize\n",
    "final_data.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Correlation\n",
    "\n",
    "Let's make sure we don't have multicollinearity before we proceed. Guidelines for Pearson Correlation Coefficient:\n",
    "\n",
    "- .00-.19 (very weak correlation)\n",
    "- .20-.39 (weak correlation)\n",
    "- .40-.59 (moderate correlation)\n",
    "- .60-.79 (strong correlation)\n",
    "- .80-1.0 (very strong correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_corr = Correlation.corr(final_data, 'features', 'pearson').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result to an array\n",
    "array_corr = coefficients_corr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.27521591, -0.39746734, -0.08158675,  0.09238617,\n",
       "        -0.10934899, -0.22271785,  0.08194602],\n",
       "       [-0.27521591,  1.        , -0.3235799 ,  0.10725203,  0.04327042,\n",
       "        -0.28399861, -0.28160267, -0.04424602],\n",
       "       [-0.39746734, -0.3235799 ,  1.        , -0.25698402,  0.37750315,\n",
       "        -0.00996083,  0.07910849, -0.15437052],\n",
       "       [-0.08158675,  0.10725203, -0.25698402,  1.        , -0.65753291,\n",
       "        -0.1822936 , -0.45066117,  0.27761822],\n",
       "       [ 0.09238617,  0.04327042,  0.37750315, -0.65753291,  1.        ,\n",
       "        -0.26599915,  0.22269123, -0.19270003],\n",
       "       [-0.10934899, -0.28399861, -0.00996083, -0.1822936 , -0.26599915,\n",
       "         1.        , -0.17848096, -0.00301588],\n",
       "       [-0.22271785, -0.28160267,  0.07910849, -0.45066117,  0.22269123,\n",
       "        -0.17848096,  1.        , -0.1560947 ],\n",
       "       [ 0.08194602, -0.04424602, -0.15437052,  0.27761822, -0.19270003,\n",
       "        -0.00301588, -0.1560947 ,  1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08194602387182176\n",
      "-0.044246019304454175\n",
      "-0.15437051606792915\n",
      "0.27761822152100296\n",
      "-0.19270002804347258\n",
      "-0.0030158803467436645\n",
      "-0.15609470264758615\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Correlation between attributes and the target variable\n",
    "for item in array_corr:\n",
    "    print(item[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30\n",
    "training_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML (Automated Machine Learning)\n",
    "\n",
    "Let's create a function to automate the use of different algorithms. This function will create, train and evaluate each of them with different combinations of hyperparameters. And then we will choose the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Module\n",
    "def func_module_ml(regression_algorithm):\n",
    "\n",
    "     # Function to get the regression algorithm type and create the object instance\n",
    "    def func_alg_type(alg_regression):\n",
    "        algorithm = alg_regression\n",
    "        alg_type = type(algorithm).__name__\n",
    "        return alg_type\n",
    "    \n",
    "    # Apply the previous function\n",
    "    alg_type = func_alg_type(regression_algorithm)\n",
    "\n",
    "    # If the algorithm is Linear Regression, we enter this if block\n",
    "    if alg_type == \"LinearRegression\":\n",
    "        \n",
    "        # We trained the first version of the model without cross-validation\n",
    "        model = regressor.fit(training_data)\n",
    "        \n",
    "        # Model metrics\n",
    "        print('\\033[1m' + \"Linear Regression Model Without Cross Validation:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Evaluate the model with test data\n",
    "        test_result = model.evaluate(training_data)\n",
    "\n",
    "        # Print model error metrics with test data\n",
    "        print(\"RMSE in Test: {}\".format(test_result.rootMeanSquaredError))\n",
    "        print(\"R2 Coefficient in Test: {}\".format(test_result.r2))\n",
    "        print(\"\")\n",
    "        \n",
    "        # Creating the second version of the model with the same algorithm, but using cross validation\n",
    "        \n",
    "        # Prepare the hyperparameter grid\n",
    "        paramGrid = (ParamGridBuilder().addGrid(regressor.regParam, [0.1, 0.01]).build())\n",
    "        \n",
    "        # Create the evaluators\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Create the Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        print('\\033[1m' + \"Linear Regression Model With Cross Validation:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Train the model with cross validation\n",
    "        model = crossval.fit(training_data)\n",
    "        \n",
    "        # Save the best model from version 2\n",
    "        global LR_BestModel \n",
    "        LR_BestModel = model.bestModel\n",
    "                \n",
    "        # Predictions with test data\n",
    "        predictions = LR_BestModel.transform(training_data)\n",
    "        \n",
    "        # Evaluate the best model\n",
    "        test_result_rmse = eval_rmse.evaluate(predictions)\n",
    "        print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "        test_result_r2 = eval_r2.evaluate(predictions)\n",
    "        print('R2 Coefficient in Test:', test_result_r2)\n",
    "        print(\"\")\n",
    "    \n",
    "        # List of columns to place in the summary dataframe\n",
    "        columns = ['Regressor', 'Result_RMSE', 'Result_R2']\n",
    "        \n",
    "        # Format the results and create the dataframe\n",
    "        \n",
    "        # Format the metrics and algorithm name\n",
    "        rmse_str = [str(test_result_rmse)] \n",
    "        r2_str = [str(test_result_r2)] \n",
    "        alg_type = [alg_type] \n",
    "        \n",
    "        # Create the dataframne\n",
    "        df_result = spark.createDataFrame(zip(alg_type, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Saves the results to the dataframe\n",
    "        df_result = df_result.withColumn('Result_RMSE', df_result.Result_RMSE.substr(0, 5))\n",
    "        df_result = df_result.withColumn('Result_R2', df_result.Result_R2.substr(0, 5))\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Verify if the algorithm is the Decision Tree and we create the hyperparameter grid\n",
    "        if alg_type in(\"DecisionTreeRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.maxBins, [10, 20, 40]).build())\n",
    "\n",
    "        # Verify if the algorithm is the Random Forest and we create the hyperparameter grid\n",
    "        if alg_type in(\"RandomForestRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.numTrees, [5, 20]).build())\n",
    "\n",
    "        # Verify if the algorithm is the GBT and we create the hyperparameter grid\n",
    "        if alg_type in(\"GBTRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20]) \\\n",
    "                         .addGrid(regressor.maxIter, [10, 15])\n",
    "                         .build())\n",
    "            \n",
    "        # Verify if the algorithm is Isotonic \n",
    "        if alg_type in(\"IsotonicRegression\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.isotonic, [True, False]).build())\n",
    "\n",
    "        # Create the evaluators\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Prepare the Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        # Train the model using cross validation\n",
    "        model = crossval.fit(training_data)\n",
    "        \n",
    "        # Extract the best model\n",
    "        BestModel = model.bestModel\n",
    "\n",
    "        # Summary of each model\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"DecisionTreeRegressor\"):\n",
    "            \n",
    "            # Global variable\n",
    "            global DT_BestModel \n",
    "            DT_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test data\n",
    "            predictions_DT = DT_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"Decision Tree Model With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(predictions_DT)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(predictions_DT)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"RandomForestRegressor\"):\n",
    "            \n",
    "            # Global variable\n",
    "            global RF_BestModel \n",
    "            RF_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test data\n",
    "            predictions_RF = RF_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"RandomForest Model With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(predictions_RF)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(predictions_RF)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Model metrics\n",
    "        if alg_type in(\"GBTRegressor\"):\n",
    "\n",
    "            # Global variable\n",
    "            global GBT_BestModel \n",
    "            GBT_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test data\n",
    "            predictions_GBT = GBT_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"Gradient-Boosted Tree (GBT) Model With Cross-Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(predictions_GBT)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(predictions_GBT)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Model metrics\n",
    "        if alg_type in(\"IsotonicRegression\"):\n",
    "\n",
    "            # Global variable\n",
    "            global ISO_BestModel \n",
    "            ISO_BestModel = model.bestModel\n",
    "            \n",
    "            # Predictions with test data\n",
    "            predictions_ISO = ISO_BestModel.transform(test_data)\n",
    "            \n",
    "            print('\\033[1m' + \"Isotonic Model With Cross Validation:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Model evaluation\n",
    "            test_result_rmse = eval_rmse.evaluate(predictions_ISO)\n",
    "            print('RMSE in Test:', test_result_rmse)\n",
    "        \n",
    "            test_result_r2 = eval_r2.evaluate(predictions_ISO)\n",
    "            print('R2 Coefficient in Test:', test_result_r2)\n",
    "            print(\"\")\n",
    "                    \n",
    "        # List of columns to place in the summary dataframe\n",
    "        columns = ['Regressor', 'Result_RMSE', 'Result_R2']\n",
    "        \n",
    "        # Predictions with test data\n",
    "        predictions = model.transform(test_data)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        rmse = eval_rmse.evaluate(predictions)\n",
    "        rmse_str = [str(rmse)]\n",
    "        \n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        r2 = eval_r2.evaluate(predictions)\n",
    "        r2_str = [str(r2)]\n",
    "         \n",
    "        alg_type = [alg_type] \n",
    "        \n",
    "        # Create the dataframe\n",
    "        df_result = spark.createDataFrame(zip(alg_type, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Saves the result to the dataframe\n",
    "        df_result = df_result.withColumn('Result_RMSE', df_result.Result_RMSE.substr(0, 5))\n",
    "        df_result = df_result.withColumn('Result_R2', df_result.Result_R2.substr(0, 5))\n",
    "        \n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Machine Learning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "regressors = [LinearRegression(),\n",
    "               DecisionTreeRegressor(),\n",
    "               RandomForestRegressor(),\n",
    "               GBTRegressor(),\n",
    "               IsotonicRegression()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and Values\n",
    "columns = ['Regressor', 'Result_RMSE', 'Result_R2']\n",
    "values = [(\"N/A\", \"N/A\", \"N/A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the summary table\n",
    "df_training_results = spark.createDataFrame(values, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Without Cross Validation:\u001b[0m\n",
      "\n",
      "RMSE in Test: 10.319835730892946\n",
      "R2 Coefficient in Test: 0.6230042769279667\n",
      "\n",
      "\u001b[1mLinear Regression Model With Cross Validation:\u001b[0m\n",
      "\n",
      "RMSE in Test: 10.325077362303277\n",
      "R2 Coefficient in Test: 0.6226212137636511\n",
      "\n",
      "\u001b[1mDecision Tree Model With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 8.649796530833731\n",
      "R2 Coefficient in Test: 0.7225073594678377\n",
      "\n",
      "\u001b[1mRandomForest Model With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 7.61442137499598\n",
      "R2 Coefficient in Test: 0.7849628480617274\n",
      "\n",
      "\u001b[1mGradient-Boosted Tree (GBT) Model With Cross-Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 6.507985899758818\n",
      "R2 Coefficient in Test: 0.8429156595704784\n",
      "\n",
      "\u001b[1mIsotonic Model With Cross Validation:\u001b[0m\n",
      " \n",
      "RMSE in Test: 13.630390793819348\n",
      "R2 Coefficient in Test: 0.3109411716623476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for regressor in regressors:\n",
    "    \n",
    "    # For each regressor obtains the result\n",
    "    model_result = func_module_ml(regressor)\n",
    "    \n",
    "    # Save the results\n",
    "    df_training_results = df_training_results.union(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the rows != N/A\n",
    "df_training_results = df_training_results.where(\"Regressor!='N/A'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------+---------+\n",
      "|Regressor            |Result_RMSE|Result_R2|\n",
      "+---------------------+-----------+---------+\n",
      "|LinearRegression     |10.32      |0.622    |\n",
      "|DecisionTreeRegressor|8.649      |0.722    |\n",
      "|RandomForestRegressor|7.614      |0.784    |\n",
      "|GBTRegressor         |6.507      |0.842    |\n",
      "|IsotonicRegression   |13.63      |0.310    |\n",
      "+---------------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "df_training_results.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GBT model presented the best overall performance and will be used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with the Trained Model\n",
    "\n",
    "Preparing a record with new data.\n",
    "\n",
    "- Cement: 540\n",
    "- Blast Furnace Slag: 0\n",
    "- Fly Ash: 0\n",
    "- Water: 162\n",
    "- Superplasticizer: 2.5\n",
    "- Coarse Aggregate: 1040\n",
    "- Fine Aggregate: 676\n",
    "- Age: 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values\n",
    "values = [(540,0.0,0.0,162,2.5,1040,676,28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "column_names = data.columns\n",
    "column_names = column_names[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate values with column names\n",
    "new_data = spark.createDataFrame(values, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformation applied in data preparation to the \"age\" column.\n",
    "new_data = new_data.withColumn(\"age\", log(\"age\") +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_list = [\"cement\",\n",
    "                   \"slag\",\n",
    "                   \"flyash\",\n",
    "                   \"water\",\n",
    "                   \"superplasticizer\",\n",
    "                   \"coarseaggregate\",\n",
    "                   \"fineaggregate\",\n",
    "                   \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "assembler = VectorAssembler(inputCols = attributes_list, outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into vector\n",
    "new_data = assembler.transform(new_data).select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizes the data (same transformation applied to training data)\n",
    "new_data_scaled = scalerModel.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the resulting column\n",
    "new_data_final = new_data_scaled.select('scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column (MLlib requirement)\n",
    "new_data_final = new_data_final.withColumnRenamed('scaledFeatures','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with new data\n",
    "new_data_predictions = GBT_BestModel.transform(new_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[1.0,0.0,0.0,0.32...|35.456736387681936|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "new_data_predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
